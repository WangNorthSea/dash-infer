{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f520af40-1b6d-4e92-8a91-81374d5fe17d",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-28T05:37:04.128994Z",
     "iopub.status.busy": "2024-04-28T05:37:04.128813Z",
     "iopub.status.idle": "2024-04-28T05:37:04.297205Z",
     "shell.execute_reply": "2024-04-28T05:37:04.296689Z",
     "shell.execute_reply.started": "2024-04-28T05:37:04.128978Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!export LANG=C && export LC_ALL=C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14b23bff-d0e5-4889-83d2-f2951dc3af61",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-04-28T05:37:04.298163Z",
     "iopub.status.busy": "2024-04-28T05:37:04.297917Z",
     "iopub.status.idle": "2024-04-28T05:37:05.626472Z",
     "shell.execute_reply": "2024-04-28T05:37:05.625954Z",
     "shell.execute_reply.started": "2024-04-28T05:37:04.298146Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tensorflow 2.14.0\n",
      "Uninstalling tensorflow-2.14.0:\n",
      "  Successfully uninstalled tensorflow-2.14.0\n",
      "Found existing installation: tensorflow-estimator 2.14.0\n",
      "Uninstalling tensorflow-estimator-2.14.0:\n",
      "  Successfully uninstalled tensorflow-estimator-2.14.0\n",
      "Found existing installation: tensorflow-io-gcs-filesystem 0.35.0\n",
      "Uninstalling tensorflow-io-gcs-filesystem-0.35.0:\n",
      "  Successfully uninstalled tensorflow-io-gcs-filesystem-0.35.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# The tensorflow in the environment contains some CUDA-related content that can cause conflicts\n",
    "!pip uninstall -y tensorflow tensorflow-estimator tensorflow-io-gcs-filesystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8a5e69-65f6-4bc4-be94-14fc6ef17251",
   "metadata": {},
   "source": [
    "Install DashInfer and qwen model dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e132c0e5-3b5b-4200-b3eb-7a0bae419c57",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-04-28T05:37:05.627420Z",
     "iopub.status.busy": "2024-04-28T05:37:05.627184Z",
     "iopub.status.idle": "2024-04-28T05:38:01.669076Z",
     "shell.execute_reply": "2024-04-28T05:38:01.668478Z",
     "shell.execute_reply.started": "2024-04-28T05:37:05.627402Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Collecting dashinfer\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/80/ed/836e8fd62fa3f02551055e450bdd52878842153110729f7d64cac2ed02cc/dashinfer-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.1/25.1 MB\u001b[0m \u001b[31m491.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting protobuf==3.18 (from dashinfer)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/74/4e/9f3cb458266ef5cdeaa1e72a90b9eda100e3d1803cbd7ec02f0846da83c3/protobuf-3.18.0-py2.py3-none-any.whl (174 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.2/174.2 kB\u001b[0m \u001b[31m483.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'protobuf' candidate (version 3.18.0 at https://mirrors.aliyun.com/pypi/packages/74/4e/9f3cb458266ef5cdeaa1e72a90b9eda100e3d1803cbd7ec02f0846da83c3/protobuf-3.18.0-py2.py3-none-any.whl#sha256=615099e52e9fbc9fde00177267a94ca820ecf4e80093e390753568b7d8cb3c1a (from https://mirrors.aliyun.com/pypi/simple/protobuf/))\n",
      "Reason for being yanked: This version claims to support Python 2 but does not\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: protobuf, dashinfer\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "onnx 1.16.0 requires protobuf>=3.20.2, but you have protobuf 3.18.0 which is incompatible.\n",
      "pai-easycv 0.11.6 requires timm==0.5.4, but you have timm 0.9.16 which is incompatible.\n",
      "tensorboard 2.16.2 requires protobuf!=4.24.0,>=3.19.6, but you have protobuf 3.18.0 which is incompatible.\n",
      "tensorboardx 2.6.2.2 requires protobuf>=3.20, but you have protobuf 3.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dashinfer-1.0.2 protobuf-3.18.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.29.3)\n",
      "Requirement already satisfied: transformers_stream_generator in /opt/conda/lib/python3.10/site-packages (0.0.5)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.7)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2+cpu)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.1)\n",
      "Requirement already satisfied: transformers>=4.26.1 in /opt/conda/lib/python3.10/site-packages (from transformers_stream_generator) (4.38.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.12.2)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.26.1->transformers_stream_generator) (0.15.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.26.1->transformers_stream_generator) (4.65.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install dashinfer\n",
    "!pip install dashinfer\n",
    "\n",
    "# install model dependencies\n",
    "!pip install sentencepiece accelerate transformers_stream_generator tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9587b317-af79-4352-a73e-b603275d3501",
   "metadata": {},
   "source": [
    "Define some functions to download models, organize prompts and print output text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a002c918-d639-409e-9ab9-14a112bcde26",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-28T05:38:01.670783Z",
     "iopub.status.busy": "2024-04-28T05:38:01.670509Z",
     "iopub.status.idle": "2024-04-28T05:38:03.378816Z",
     "shell.execute_reply": "2024-04-28T05:38:03.378274Z",
     "shell.execute_reply.started": "2024-04-28T05:38:01.670763Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Copyright (c) Alibaba, Inc. and its affiliates.\n",
    "# @file    basic_example_qwen_v10_io.py\n",
    "#\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import time\n",
    "import queue\n",
    "import random\n",
    "import subprocess\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# os.environ['GLOG_minloglevel'] = '2' # disable LOG(INFO) logging\n",
    "from dashinfer.helper import EngineHelper\n",
    "\n",
    "\n",
    "def download_model(model_id, revision, source=\"modelscope\"):\n",
    "    print(f\"Downloading model {model_id} (revision: {revision}) from {source}\")\n",
    "    if source == \"modelscope\":\n",
    "        from modelscope import snapshot_download\n",
    "        model_dir = snapshot_download(model_id, revision=revision)\n",
    "    elif source == \"huggingface\":\n",
    "        from huggingface_hub import snapshot_download\n",
    "        model_dir = snapshot_download(repo_id=model_id)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown source\")\n",
    "\n",
    "    print(f\"Save model to path {model_dir}\")\n",
    "\n",
    "    return model_dir\n",
    "\n",
    "\n",
    "def create_test_prompt(inputs, default_gen_cfg=None):\n",
    "    gen_cfg_list = []\n",
    "    prompt = copy.deepcopy(inputs)\n",
    "    prompt = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n\" \\\n",
    "                     + prompt + \"<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    if default_gen_cfg != None:\n",
    "        gen_cfg = copy.deepcopy(default_gen_cfg)\n",
    "        gen_cfg[\"seed\"] = random.randint(0, 10000)\n",
    "        gen_cfg_list.append(gen_cfg)\n",
    "\n",
    "    return [prompt], gen_cfg_list\n",
    "\n",
    "\n",
    "def print_in_place(generator, user_input):\n",
    "    for part in generator:\n",
    "        clear_output(wait=True)  # 清除当前的输出，并等待新的输出\n",
    "        print(f\"User: {user_input}\")\n",
    "        print(f\"Answer:\\n{part}\")\n",
    "        sys.stdout.flush()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe09c227-f2e4-4566-85e6-4763d47e6fce",
   "metadata": {},
   "source": [
    "Use a Dict() to keep inference parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e57cbbc5-654f-492e-8f29-674ffc5a638c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-28T05:38:03.379838Z",
     "iopub.status.busy": "2024-04-28T05:38:03.379495Z",
     "iopub.status.idle": "2024-04-28T05:38:03.383402Z",
     "shell.execute_reply": "2024-04-28T05:38:03.382989Z",
     "shell.execute_reply.started": "2024-04-28T05:38:03.379820Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"model_name\": \"Qwen-1_8B-Chat\",\n",
    "  \"model_type\": \"Qwen_v10\",\n",
    "  \"model_path\": \"./dashinfer_models/\",\n",
    "  \"data_type\": \"float32\",\n",
    "  \"device_type\": \"CPU\",\n",
    "  \"device_ids\": [0],\n",
    "  \"multinode_mode\": False,\n",
    "  \"convert_config\": {\n",
    "    \"do_dynamic_quantize_convert\": False\n",
    "  },\n",
    "  \"engine_config\": {\n",
    "    \"engine_max_length\": 2048,\n",
    "    \"engine_max_batch\": 8,\n",
    "    \"do_profiling\": False,\n",
    "    \"num_threads\": 0,\n",
    "    \"matmul_precision\": \"medium\"\n",
    "  },\n",
    "  \"generation_config\": {\n",
    "    \"temperature\": 1.0,\n",
    "    \"early_stopping\": True,\n",
    "    \"top_k\": 1024,\n",
    "    \"top_p\": 0.8,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"presence_penalty\": 0.0,\n",
    "    \"min_length\": 0,\n",
    "    \"max_length\": 2048,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"eos_token_id\": 151643,\n",
    "    \"seed\": 1234,\n",
    "    \"stop_words_ids\": [[151643], [151644], [151645]]\n",
    "  },\n",
    "  \"quantization_config\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8147da86-caa6-4891-8910-6fefb9727540",
   "metadata": {},
   "source": [
    "Set ENV for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8de5fb59-c3e0-4977-b248-38a7ba767340",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T05:38:03.384357Z",
     "iopub.status.busy": "2024-04-28T05:38:03.384032Z",
     "iopub.status.idle": "2024-04-28T05:38:06.267734Z",
     "shell.execute_reply": "2024-04-28T05:38:06.267205Z",
     "shell.execute_reply.started": "2024-04-28T05:38:03.384341Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cmd = f\"pip show dashinfer | grep 'Location' | cut -d ' ' -f 2\"\n",
    "package_location = subprocess.run(cmd,\n",
    "                                  stdout=subprocess.PIPE,\n",
    "                                  stderr=subprocess.PIPE,\n",
    "                                  shell=True,\n",
    "                                  text=True)\n",
    "package_location = package_location.stdout.strip()\n",
    "os.environ[\"AS_DAEMON_PATH\"] = package_location + \"/dashinfer/allspark/bin\"\n",
    "os.environ[\"AS_NUMA_NUM\"] = str(len(config[\"device_ids\"]))\n",
    "os.environ[\"AS_NUMA_OFFSET\"] = str(config[\"device_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07864f6e-8d70-419b-8c7d-e420b8f3ebe0",
   "metadata": {},
   "source": [
    "Download models from modelscope or huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "170ac1fe-b4f3-4e57-befa-52bedc94dcbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T05:38:06.268635Z",
     "iopub.status.busy": "2024-04-28T05:38:06.268400Z",
     "iopub.status.idle": "2024-04-28T05:38:47.203975Z",
     "shell.execute_reply": "2024-04-28T05:38:47.203337Z",
     "shell.execute_reply.started": "2024-04-28T05:38:06.268619Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 13:38:06,294 - modelscope - INFO - PyTorch version 2.1.2+cpu Found.\n",
      "2024-04-28 13:38:06,296 - modelscope - INFO - Loading ast index from /mnt/workspace/.cache/modelscope/ast_indexer\n",
      "2024-04-28 13:38:06,296 - modelscope - INFO - No valid ast index found from /mnt/workspace/.cache/modelscope/ast_indexer, generating ast index from prebuilt!\n",
      "2024-04-28 13:38:06,343 - modelscope - INFO - Loading done! Current index file version is 1.14.0, with md5 aacbf9e8ebe525a5896d4c89570c0097 and a total number of 976 components indexed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model qwen/Qwen-1_8B-Chat (revision: v1.0.0) from modelscope\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/beartype/_util/error/utilerrwarn.py:67: BeartypeModuleUnimportableWarning: Ignoring module \"onnx\" importation exception:\n",
      "    ImportError: cannot import name 'builder' from 'google.protobuf.internal' (/opt/conda/lib/python3.10/site-packages/google/protobuf/internal/__init__.py)\n",
      "  warn(message, cls)\n",
      "/opt/conda/lib/python3.10/site-packages/beartype/_util/error/utilerrwarn.py:67: BeartypeModuleUnimportableWarning: Ignoring module \"onnx\" importation exception:\n",
      "    ImportError: cannot import name 'builder' from 'google.protobuf.internal' (/opt/conda/lib/python3.10/site-packages/google/protobuf/internal/__init__.py)\n",
      "  warn(message, cls)\n",
      "/opt/conda/lib/python3.10/site-packages/beartype/_util/error/utilerrwarn.py:67: BeartypeModuleUnimportableWarning: Ignoring module \"onnx\" importation exception:\n",
      "    ImportError: cannot import name 'builder' from 'google.protobuf.internal' (/opt/conda/lib/python3.10/site-packages/google/protobuf/internal/__init__.py)\n",
      "  warn(message, cls)\n",
      "2024-04-28 13:38:07,505 - modelscope - INFO - Use user-specified model revision: v1.0.0\n",
      "Downloading: 100%|██████████| 8.21k/8.21k [00:00<00:00, 30.9MB/s]\n",
      "Downloading: 100%|██████████| 50.8k/50.8k [00:00<00:00, 14.5MB/s]\n",
      "Downloading: 100%|██████████| 244k/244k [00:00<00:00, 13.8MB/s]\n",
      "Downloading: 100%|██████████| 135k/135k [00:00<00:00, 10.6MB/s]\n",
      "Downloading: 100%|██████████| 910/910 [00:00<00:00, 7.30MB/s]\n",
      "Downloading: 100%|██████████| 77.0/77.0 [00:00<00:00, 623kB/s]\n",
      "Downloading: 100%|██████████| 2.29k/2.29k [00:00<00:00, 19.6MB/s]\n",
      "Downloading: 100%|██████████| 1.88k/1.88k [00:00<00:00, 15.9MB/s]\n",
      "Downloading: 100%|██████████| 249/249 [00:00<00:00, 2.20MB/s]\n",
      "Downloading: 100%|██████████| 1.63M/1.63M [00:00<00:00, 38.8MB/s]\n",
      "Downloading: 100%|██████████| 1.84M/1.84M [00:00<00:00, 146MB/s]\n",
      "Downloading: 100%|██████████| 2.64M/2.64M [00:00<00:00, 28.8MB/s]\n",
      "Downloading: 100%|██████████| 7.11k/7.11k [00:00<00:00, 24.7MB/s]\n",
      "Downloading: 100%|██████████| 80.8k/80.8k [00:00<00:00, 103MB/s]\n",
      "Downloading: 100%|██████████| 80.8k/80.8k [00:00<00:00, 91.1MB/s]\n",
      "Downloading: 100%|█████████▉| 1.90G/1.90G [00:05<00:00, 340MB/s]\n",
      "Downloading: 100%|█████████▉| 1.52G/1.52G [00:04<00:00, 356MB/s]\n",
      "Downloading: 100%|██████████| 14.4k/14.4k [00:00<00:00, 13.3MB/s]\n",
      "Downloading: 100%|██████████| 54.3k/54.3k [00:00<00:00, 14.2MB/s]\n",
      "Downloading: 100%|██████████| 15.0k/15.0k [00:00<00:00, 9.42MB/s]\n",
      "Downloading: 100%|██████████| 237k/237k [00:00<00:00, 21.1MB/s]\n",
      "Downloading: 100%|██████████| 116k/116k [00:00<00:00, 13.2MB/s]\n",
      "Downloading: 100%|██████████| 2.44M/2.44M [00:00<00:00, 59.0MB/s]\n",
      "Downloading: 100%|██████████| 473k/473k [00:00<00:00, 25.8MB/s]\n",
      "Downloading: 100%|██████████| 14.3k/14.3k [00:00<00:00, 14.3MB/s]\n",
      "Downloading: 100%|██████████| 79.0k/79.0k [00:00<00:00, 8.56MB/s]\n",
      "Downloading: 100%|██████████| 46.4k/46.4k [00:00<00:00, 7.48MB/s]\n",
      "Downloading: 100%|██████████| 0.98M/0.98M [00:00<00:00, 25.5MB/s]\n",
      "Downloading: 100%|██████████| 205k/205k [00:00<00:00, 38.7MB/s]\n",
      "Downloading: 100%|██████████| 19.4k/19.4k [00:00<00:00, 4.86MB/s]\n",
      "Downloading: 100%|██████████| 302k/302k [00:00<00:00, 21.7MB/s]\n",
      "Downloading: 100%|██████████| 615k/615k [00:00<00:00, 27.9MB/s]\n",
      "Downloading: 100%|██████████| 376k/376k [00:00<00:00, 20.4MB/s]\n",
      "Downloading: 100%|██████████| 445k/445k [00:00<00:00, 13.8MB/s]\n",
      "Downloading: 100%|██████████| 25.9k/25.9k [00:00<00:00, 8.96MB/s]\n",
      "Downloading: 100%|██████████| 395k/395k [00:00<00:00, 40.7MB/s]\n",
      "Downloading: 100%|██████████| 176k/176k [00:00<00:00, 12.8MB/s]\n",
      "Downloading: 100%|██████████| 182k/182k [00:00<00:00, 10.3MB/s]\n",
      "Downloading: 100%|██████████| 824k/824k [00:00<00:00, 22.8MB/s]\n",
      "Downloading: 100%|██████████| 426k/426k [00:00<00:00, 16.2MB/s]\n",
      "Downloading: 100%|██████████| 433k/433k [00:00<00:00, 23.7MB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 21.3MB/s]\n",
      "Downloading: 100%|██████████| 403k/403k [00:00<00:00, 76.8MB/s]\n",
      "Downloading: 100%|██████████| 9.39k/9.39k [00:00<00:00, 35.4MB/s]\n",
      "Downloading: 100%|██████████| 403k/403k [00:00<00:00, 55.0MB/s]\n",
      "Downloading: 100%|██████████| 79.0k/79.0k [00:00<00:00, 11.1MB/s]\n",
      "Downloading: 100%|██████████| 173/173 [00:00<00:00, 1.44MB/s]\n",
      "Downloading: 100%|██████████| 41.9k/41.9k [00:00<00:00, 79.3MB/s]\n",
      "Downloading: 100%|██████████| 230k/230k [00:00<00:00, 15.6MB/s]\n",
      "Downloading: 100%|██████████| 1.27M/1.27M [00:00<00:00, 30.4MB/s]\n",
      "Downloading: 100%|██████████| 664k/664k [00:00<00:00, 29.6MB/s]\n",
      "Downloading: 100%|██████████| 404k/404k [00:00<00:00, 58.5MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model to path /mnt/workspace/.cache/modelscope/qwen/Qwen-1_8B-Chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## download original model\n",
    "## download model from huggingface\n",
    "# original_model = {\n",
    "#     \"source\": \"huggingface\",\n",
    "#     \"model_id\": \"Qwen/Qwen-1_8B-Chat\",\n",
    "#     \"revision\": \"\",\n",
    "#     \"model_path\": \"\"\n",
    "# }\n",
    "\n",
    "## download model from modelscope\n",
    "original_model = {\n",
    "    \"source\": \"modelscope\",\n",
    "    \"model_id\": \"qwen/Qwen-1_8B-Chat\",\n",
    "    \"revision\": \"v1.0.0\",\n",
    "    \"model_path\": \"\"\n",
    "}\n",
    "original_model[\"model_path\"] = download_model(original_model[\"model_id\"],\n",
    "                                              original_model[\"revision\"],\n",
    "                                              original_model[\"source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fba360-b05b-4fcd-bed9-84762c664aba",
   "metadata": {},
   "source": [
    "Initialize DashInfer engine.\n",
    "- Huggingface models will be converted to DashInfer models at the initial run.\n",
    "- In init_engine(), DashInfer will warm-up with random numbers, which takes some time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5d478a2-366d-4479-8ae0-86d8139b1813",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-04-28T05:38:47.205297Z",
     "iopub.status.busy": "2024-04-28T05:38:47.204859Z",
     "iopub.status.idle": "2024-04-28T05:40:02.903037Z",
     "shell.execute_reply": "2024-04-28T05:40:02.902472Z",
     "shell.execute_reply.started": "2024-04-28T05:38:47.205268Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "I20240428 13:38:47.209024   588 as_engine.cpp:226] AllSpark Init with Version: 1.0.2/(GitSha1:3a5cfb7a-dirty)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### convert_config: {'do_dynamic_quantize_convert': False}\n",
      "### engine_config: {'engine_max_length': 2048, 'engine_max_batch': 8, 'do_profiling': False, 'num_threads': 0, 'matmul_precision': 'medium'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.59s/it]\n",
      "E20240428 13:38:57.298009   588 as_engine.cpp:924] workers is empty\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No such file or directory: ./dashinfer_models/Qwen-1_8B-Chat_cpu_single_float32.asgraph\n",
      "\n",
      "trans model from huggingface model: /mnt/workspace/.cache/modelscope/qwen/Qwen-1_8B-Chat\n",
      "Dashinfer model will save to  ./dashinfer_models/\n",
      "### model_config: {'vocab_size': 151936, 'hidden_size': 2048, 'intermediate_size': 11008, 'num_hidden_layers': 24, 'num_attention_heads': 16, 'emb_dropout_prob': 0.0, 'attn_dropout_prob': 0.0, 'layer_norm_epsilon': 1e-06, 'initializer_range': 0.02, 'scale_attn_weights': True, 'use_cache': True, 'max_position_embeddings': 8192, 'bf16': False, 'fp16': False, 'fp32': True, 'kv_channels': 128, 'rotary_pct': 1.0, 'rotary_emb_base': 10000, 'use_dynamic_ntk': True, 'use_logn_attn': True, 'use_flash_attn': False, 'no_bias': True, 'use_cache_quantization': False, 'use_cache_kernel': False, 'softmax_in_fp32': False, 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': None, 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': False, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['QWenLMHeadModel'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': 'QWenTokenizer', 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': '/mnt/workspace/.cache/modelscope/qwen/Qwen-1_8B-Chat', '_commit_hash': None, '_attn_implementation_internal': None, 'transformers_version': '4.32.0', 'auto_map': {'AutoConfig': 'configuration_qwen.QWenConfig', 'AutoModelForCausalLM': 'modeling_qwen.QWenLMHeadModel'}, 'model_type': 'qwen', 'onnx_safe': None, 'seq_length': 8192, 'size_per_head': 128}\n",
      "save asgraph to  ./dashinfer_models/Qwen-1_8B-Chat_cpu_single_float32.asgraph\n",
      "save asparam to  ./dashinfer_models/Qwen-1_8B-Chat_cpu_single_float32.asparam\n",
      "parse weight time:  8.823164463043213\n",
      "current allspark version major[ 1 ] minor[ 0 ] patch[ 2 ] commit =  3a5cfb7a\n",
      "calculate md5 of asgraph =  2d5d46b154e9f0c8bd51274775675eea\n",
      "torch build meta: \t model_name \t:  Qwen-1_8B-Chat_cpu_single_float32\n",
      "torch build meta: \t model_type \t:  Qwen_v10\n",
      "torch build meta: \t save_dir \t:  ./dashinfer_models/\n",
      "torch build meta: \t multinode_mode \t:  False\n",
      "torch build meta: \t data_type \t:  float32\n",
      "torch build meta: \t do_dynamic_quantize_convert \t:  False\n",
      "torch build meta: \t use_dynamic_ntk \t:  True\n",
      "torch build meta: \t use_logn_attn \t:  True\n",
      "torch build meta: \t model_sequence_length \t:  2048\n",
      "torch build meta: \t seqlen_extrapolation \t:  1.0\n",
      "torch build meta: \t rotary_base \t:  10000\n",
      "serialize_model_from_torch: save model = true, time :  8.912247657775879\n",
      "convert model from HF finished, build time is 8.912510633468628 seconds\n",
      "Error parse lscpu ouput: list index out of range, use default value 0\n",
      "build model over, build time is 3.8234972953796387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I20240428 13:39:06.234314   588 as_engine.cpp:378] Build model use following config:\n",
      "AsModelConfig :\n",
      "\tmodel_name: Qwen-1_8B-Chat_cpu_single_float32\n",
      "\tmodel_path: ./dashinfer_models/Qwen-1_8B-Chat_cpu_single_float32.asgraph\n",
      "\tweights_path: ./dashinfer_models/Qwen-1_8B-Chat_cpu_single_float32.asparam\n",
      "\tcompute_unit: CPU:0\n",
      "\tnum_threads: 0\n",
      "\tmatmul_precision: medium\n",
      "\tprefill_mode: AsPrefillDefault\n",
      "\tcache_mode: AsCacheDefault\n",
      "\tengine_max_length = 2048\n",
      "\tengine_max_batch = 8\n",
      "\n",
      "I20240428 13:39:06.234341   588 as_engine.cpp:382] Load model from : ./dashinfer_models/Qwen-1_8B-Chat_cpu_single_float32.asgraph\n",
      "I20240428 13:39:06.234362   588 as_engine.cpp:297] SetDeviceIds: DeviceIDs.size() 1\n",
      "I20240428 13:39:06.234364   588 as_engine.cpp:304] Start create 1 Device: CPU workers.\n",
      "I20240428 13:39:06.234476   886 cpu_context.cpp:114] CPUContext::InitMCCL() rank: 0 nRanks: 1\n",
      "I20240428 13:39:06.287034   588 as_param_check.hpp:342] AsParamGuard check level = CHECKER_NORMAL. Engine version = 1.0 . Weight version = 1.0 . \n",
      "I20240428 13:39:06.289023   887 as_engine.cpp:510] Start Build model for rank: 0\n",
      "I20240428 13:39:06.289038   887 weight_manager.cpp:131] Start Loading weight for model RankInfo[0/1]\n",
      "I20240428 13:39:06.289052   887 weight_manager.cpp:52] Start open model file ./dashinfer_models/Qwen-1_8B-Chat_cpu_single_float32.asparam\n",
      "I20240428 13:39:06.289062   887 weight_manager.cpp:59] Open model file success. \n",
      "I20240428 13:39:06.289655   887 weight_manager.cpp:107] Weight file header parse success...195 weight tensors are going to load. \n",
      "I20240428 13:39:10.032027   887 weight_manager.cpp:263] finish weight load for model RankInfo[0/1] time  spend: 3.742 seconds.\n",
      "I20240428 13:39:10.033531   887 as_engine.cpp:514] Finish Build model for rank: 0\n",
      "I20240428 13:39:10.035272   588 as_engine.cpp:666] StartModel: warming up...\n",
      "I20240428 13:39:10.035297   890 as_engine.cpp:1616] | AllsparkStat | Req: Running: 0 Pending: 0 \t Prompt: 0 T/s  Gen: 0 T/s \n",
      "I20240428 13:39:47.722589   797 model.cpp:430] RunDecoderContext() Success ID: 0000000000000000000000000000000\n",
      "I20240428 13:39:47.978621   797 model.cpp:482] Stop request with request id: 0000000000000000000000000000000\n",
      "I20240428 13:39:47.978683   890 as_engine.cpp:1581] [Qwen-1_8B-Chat_cpu_single_float32] request finished with uuid: 0000000000000000000000000000000\n",
      "I20240428 13:39:47.978716   890 as_engine.cpp:1616] | AllsparkStat | Req: Running: 0 Pending: 0 \t Prompt: 53.9224 T/s  Gen: 0.0527101 T/s \n",
      "I20240428 13:39:47.978811   890 as_engine.cpp:1616] | AllsparkStat | Req: Running: 0 Pending: 0 \t Prompt: 0 T/s  Gen: 0 T/s \n",
      "I20240428 13:39:47.978873   890 as_engine.cpp:1616] | AllsparkStat | Req: Running: 0 Pending: 0 \t Prompt: 0 T/s  Gen: 0 T/s \n",
      "I20240428 13:39:48.398344   797 model.cpp:430] RunDecoderContext() Success ID: 0000000000000000000000000000001\n",
      "I20240428 13:39:48.649374   890 as_engine.cpp:1616] | AllsparkStat | Req: Running: 1 Pending: 0 \t Prompt: 7.4572 T/s  Gen: 2.98288 T/s \n",
      "I20240428 13:39:49.063675   797 model.cpp:430] RunDecoderContext() Success ID: 0000000000000000000000000000002\n",
      "I20240428 13:39:49.455785   890 as_engine.cpp:1616] | AllsparkStat | Req: Running: 2 Pending: 0 \t Prompt: 6.20031 T/s  Gen: 3.72019 T/s \n",
      "I20240428 13:39:49.852830   797 model.cpp:430] RunDecoderContext() Success ID: 0000000000000000000000000000003\n",
      "I20240428 13:39:50.281210   890 as_engine.cpp:1616] | AllsparkStat | Req: Running: 3 Pending: 0 \t Prompt: 6.05748 T/s  Gen: 4.84598 T/s \n",
      "I20240428 13:39:50.675730   797 model.cpp:430] RunDecoderContext() Success ID: 0000000000000000000000000000004\n",
      "I20240428 13:39:51.206521   890 as_engine.cpp:1616] | AllsparkStat | Req: Running: 4 Pending: 0 \t Prompt: 5.40359 T/s  Gen: 5.40359 T/s \n",
      "I20240428 13:39:51.642920   797 model.cpp:430] RunDecoderContext() Success ID: 0000000000000000000000000000005\n",
      "I20240428 13:39:52.245191   890 as_engine.cpp:1616] | AllsparkStat | Req: Running: 5 Pending: 0 \t Prompt: 4.81384 T/s  Gen: 5.77661 T/s \n",
      "I20240428 13:39:52.641470   797 model.cpp:430] RunDecoderContext() Success ID: 0000000000000000000000000000006\n",
      "I20240428 13:39:53.754009   797 model.cpp:430] RunDecoderContext() Success ID: 0000000000000000000000000000007\n",
      "I20240428 13:39:54.952073   797 model.cpp:430] RunDecoderContext() Success ID: 0000000000000000000000000000008\n",
      "I20240428 13:39:55.870285   890 as_engine.cpp:1616] | AllsparkStat | Req: Running: 8 Pending: 0 \t Prompt: 4.13782 T/s  Gen: 6.62052 T/s \n",
      "I20240428 13:39:59.297385   797 model.cpp:482] Stop request with request id: 0000000000000000000000000000001\n",
      "I20240428 13:39:59.299513   890 as_engine.cpp:1581] [Qwen-1_8B-Chat_cpu_single_float32] request finished with uuid: 0000000000000000000000000000001\n",
      "I20240428 13:40:00.066485   797 model.cpp:482] Stop request with request id: 0000000000000000000000000000002\n",
      "I20240428 13:40:00.068713   890 as_engine.cpp:1581] [Qwen-1_8B-Chat_cpu_single_float32] request finished with uuid: 0000000000000000000000000000002\n",
      "I20240428 13:40:00.068732   890 as_engine.cpp:1616] | AllsparkStat | Req: Running: 6 Pending: 0 \t Prompt: 0 T/s  Gen: 9.28913 T/s \n",
      "I20240428 13:40:00.747191   797 model.cpp:482] Stop request with request id: 0000000000000000000000000000003\n",
      "I20240428 13:40:00.749341   890 as_engine.cpp:1581] [Qwen-1_8B-Chat_cpu_single_float32] request finished with uuid: 0000000000000000000000000000003\n",
      "I20240428 13:40:01.323917   797 model.cpp:482] Stop request with request id: 0000000000000000000000000000004\n",
      "I20240428 13:40:01.326013   890 as_engine.cpp:1581] [Qwen-1_8B-Chat_cpu_single_float32] request finished with uuid: 0000000000000000000000000000004\n",
      "I20240428 13:40:01.893652   797 model.cpp:482] Stop request with request id: 0000000000000000000000000000005\n",
      "I20240428 13:40:01.895835   890 as_engine.cpp:1581] [Qwen-1_8B-Chat_cpu_single_float32] request finished with uuid: 0000000000000000000000000000005\n",
      "I20240428 13:40:02.306764   797 model.cpp:482] Stop request with request id: 0000000000000000000000000000006\n",
      "I20240428 13:40:02.308984   890 as_engine.cpp:1581] [Qwen-1_8B-Chat_cpu_single_float32] request finished with uuid: 0000000000000000000000000000006\n",
      "I20240428 13:40:02.657095   797 model.cpp:482] Stop request with request id: 0000000000000000000000000000007\n",
      "I20240428 13:40:02.659348   890 as_engine.cpp:1581] [Qwen-1_8B-Chat_cpu_single_float32] request finished with uuid: 0000000000000000000000000000007\n",
      "I20240428 13:40:02.900252   797 model.cpp:482] Stop request with request id: 0000000000000000000000000000008\n",
      "I20240428 13:40:02.900306   890 as_engine.cpp:1581] [Qwen-1_8B-Chat_cpu_single_float32] request finished with uuid: 0000000000000000000000000000008\n"
     ]
    }
   ],
   "source": [
    "## init EngineHelper class\n",
    "engine_helper = EngineHelper(config)\n",
    "engine_helper.verbose = True\n",
    "engine_helper.init_tokenizer(original_model[\"model_path\"])\n",
    "engine_helper.init_torch_model(original_model[\"model_path\"])\n",
    "\n",
    "## convert huggingface model to dashinfer model\n",
    "## only one conversion is required\n",
    "if engine_helper.check_model_exist() == False:\n",
    "    engine_helper.convert_model(original_model[\"model_path\"])\n",
    "\n",
    "## init engine\n",
    "engine_helper.init_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d3eeb5-42c8-4d0f-a039-ad9adca81b36",
   "metadata": {},
   "source": [
    "Read user inputs from terminal, and call DashInfer to generate results.\n",
    "- In this example, users can interact multiple times with the model, but history will not be involved in a new conversation.\n",
    "- Input `exit` to exit the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd80ff70-ab8b-4c7d-bf9e-d6f968d0ed0e",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-04-28T05:40:02.903960Z",
     "iopub.status.busy": "2024-04-28T05:40:02.903755Z",
     "iopub.status.idle": "2024-04-28T05:41:47.391179Z",
     "shell.execute_reply": "2024-04-28T05:41:47.390650Z",
     "shell.execute_reply.started": "2024-04-28T05:40:02.903944Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: who are you\n",
      "Answer:\n",
      "I am an artificial intelligence language model designed to assist with a wide range of tasks, from answering questions and providing information to generating text and even performing creative tasks such as writing stories and songs. My purpose is to provide useful and informative responses to your inquiries to the best of my abilities based on my training data. How may I assist you today?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your inputs:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting program.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    while True:\n",
    "        input_value = input(\"Please enter your inputs: \")\n",
    "        if input_value.lower() == 'exit':\n",
    "            print(\"Exiting program.\")\n",
    "            break\n",
    "\n",
    "        prompt_list, gen_cfg_list = create_test_prompt(\n",
    "            input_value, engine_helper.default_gen_cfg)\n",
    "        request_list = engine_helper.create_request(prompt_list, gen_cfg_list)\n",
    "        request = request_list[0]\n",
    "\n",
    "        gen = engine_helper.process_one_request_stream(request)\n",
    "        print_in_place(gen, input_value)\n",
    "        time.sleep(1)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    sys.stdout.write(\"\\nProgram interrupted. Exiting...\\n\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dad5f2b-9b99-4e8a-8e18-a1181c58e8c5",
   "metadata": {},
   "source": [
    "Stop the engine and release resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da989d64-2942-4cd8-924b-8c9f1ae7d12e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T05:41:47.392053Z",
     "iopub.status.busy": "2024-04-28T05:41:47.391800Z",
     "iopub.status.idle": "2024-04-28T05:41:47.394849Z",
     "shell.execute_reply": "2024-04-28T05:41:47.394416Z",
     "shell.execute_reply.started": "2024-04-28T05:41:47.392036Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I20240428 13:41:47.392686   890 as_engine.cpp:1616] | AllsparkStat | Req: Running: 0 Pending: 0 \t Prompt: 0 T/s  Gen: 1.96627 T/s \n",
      "I20240428 13:41:47.392802   588 as_engine.cpp:841] [Qwen-1_8B-Chat_cpu_single_float32] waiting to join loop thread\n",
      "I20240428 13:41:47.392835   588 as_engine.cpp:844] [Qwen-1_8B-Chat_cpu_single_float32] loop thread joined\n"
     ]
    }
   ],
   "source": [
    "# uninit engine\n",
    "engine_helper.uninit_engine()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
